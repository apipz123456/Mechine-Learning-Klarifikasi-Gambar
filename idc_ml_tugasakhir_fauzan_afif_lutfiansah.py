# -*- coding: utf-8 -*-
"""IDC_ML_TugasAkhir_Fauzan Afif Lutfiansah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R2kNLWZD7E5_VECT0kgT-o94MFGy4e3F

**NAMA : FAUZAN AFIF LUTFIANSAH**
              **ASAL : KEBUMEN**
              **EMAIL : fauzanafiflutfiansah@gmail.com**

Syarat2 :
1. Dataset harus dibagi menjadi train set dan validation set.
2. Ukuran validation set harus 40% dari total dataset (data training memiliki 1314 sampel, dan data validasi sebanyak 874 sampel)
3. Harus mengimplementasikan augmentasi gambar.
4. Menggunakan image data generator.
5. Model harus menggunakan model sequential.
6. Pelatihan model tidak melebihi waktu 30 menit.
7. Program dikerjakan pada Google Colaboratory.
8. Akurasi dari model minimal 85%.
"""

import zipfile
import os
import PIL
import numpy as np
import tensorflow as tf
import shutil
from PIL import Image, ImageOps
import numpy as np
from google.colab import files
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

!wget --no-check-certificate \
 https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \
 -O /tmp/rockpaperscissors.zip

datasets_dir='dataset_raw'
#os.mkdir(datasets_dir)

#extrak file
import zipfile
import os

local_zip = '/tmp/rockpaperscissors.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/Datasets')
zip_ref.close()

#dataset_raw(mentah)to dir
datasets_dir = os.path.join(datasets_dir,'/content/Datasets/rockpaperscissors/rps-cv-images')

classdir_list=['paper','rock','scissors']

# validation set 40% maka untuyk training 60%
training_percentage = 0.6

#bikin direktori baru
training_datasets_dir = 'training'
validation_datasets_dir ='validation'

os.mkdir(training_datasets_dir)
os.mkdir(validation_datasets_dir)

for dirname in classdir_list:
  classpath = os.path.join(datasets_dir, dirname)
  # dataset_raw/rps-cv-images/scissors

  counter = 0
  training_length = training_percentage * len(os.listdir(classpath))

  training_classpath = os.path.join(training_datasets_dir, dirname)
  # training/paper
  validation_classpath = os.path.join(validation_datasets_dir, dirname)

  os.makedirs(training_classpath, exist_ok=True)
  os.makedirs(validation_classpath,exist_ok=True)

  for filename in os.listdir(classpath):
    filepath = os.path.join(classpath, filename)
    if counter < training_length:
      shutil.move(filepath, training_classpath)
    else:
      shutil.move(filepath, validation_classpath)

    counter += 1

training_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1./255,
    zoom_range = 0.25,
    horizontal_flip = True,
    vertical_flip= True
)

validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1./255,
)

training_generator = training_datagen.flow_from_directory(
    training_datasets_dir,
    target_size = (150, 150),
    class_mode = 'categorical'
)
validation_generator = validation_datagen.flow_from_directory(
    validation_datasets_dir,
    target_size = (150, 150),
    class_mode = 'categorical'
)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(8, (3, 3), input_shape = (150, 150, 3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(16, (3, 3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation = 'relu'),
    tf.keras.layers.Dense(3, activation = 'softmax')
])

model.compile(
    metrics = ['acc'],
    loss='categorical_crossentropy',
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
)

model.fit(
    training_generator,
    validation_data = validation_generator,
    epochs=6
)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():

  # prediksi
  path = fn
  img = image.load_img(path, target_size=(150, 150))

  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
  images = np.vstack([x])

  classes = model.predict(images, batch_size=10)
  print(fn)

  # mencari kelas prob
  predicted_class = np.argmax(classes)

  if predicted_class == 0:
    print('Batu')
  elif predicted_class == 1:
    print('Kertas')
  else:
    print('Gunting')